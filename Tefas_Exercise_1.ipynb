{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYbDLqwnUMGGYBWIEBnc+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skolix15/Machine_Learning_2025/blob/main/Tefas_Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Classifier Class"
      ],
      "metadata": {
        "id": "KTLBVu7ToHxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTFvBpoumk_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "# Used for classification => predicting discrete labels (e.g., cat vs dog, spam vs not spam).\n",
        "# In classification, a higher C => fewer misclassifications but risk of overfitting.\n",
        "class SVMClassifier:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Hyperparameter grid\n",
        "        self.param_grid = {\n",
        "            \"C\": [0.1, 8, 50, 100],\n",
        "            # \"kernel\": [\"rbf\", \"poly\", \"linear\"],\n",
        "            # \"kernel\": [\"rbf\", \"linear\"],\n",
        "            # \"kernel\": [\"rbf\"],\n",
        "            \"kernel\": [\"linear\"],\n",
        "            \"gamma\": [\"scale\", \"auto\"],\n",
        "            # \"gamma\": [\"auto\"],\n",
        "            \"decision_function_shape\": [\"ovr\", \"ovo\"],\n",
        "            # \"decision_function_shape\": [\"ovr\"],\n",
        "            # \"decision_function_shape\": [\"ovo\"],\n",
        "        }\n",
        "        self.results = []\n",
        "        self.__model = []\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.__model\n",
        "\n",
        "    @staticmethod\n",
        "    def count_correct_incorrect(y_true, y_pred):\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "        # Ensure boolean arrays\n",
        "        correct_mask = np.equal(y_true, y_pred)  # same as y_true == y_pred\n",
        "        incorrect_mask = ~correct_mask\n",
        "\n",
        "        correct = np.sum(correct_mask)\n",
        "        incorrect = np.sum(incorrect_mask)\n",
        "\n",
        "        return correct, incorrect\n",
        "\n",
        "    def evaluate_all_models(self, x_train, x_test, y_train, y_test):\n",
        "\n",
        "        # Create all parameter combinations\n",
        "        keys = list(self.param_grid.keys())\n",
        "        combinations = list(product(*self.param_grid.values()))\n",
        "\n",
        "        print(f\"Evaluating {len(combinations)} models...\\n\")\n",
        "\n",
        "        best_test_accuracy = -1\n",
        "\n",
        "        for combo in combinations:\n",
        "\n",
        "            params = dict(zip(keys, combo))\n",
        "            print(f\"[MODEL] Model with parameters {params}\\n\")\n",
        "\n",
        "            model = SVC(**params)\n",
        "\n",
        "            # Training\n",
        "            print(f\"[TRAINING] Start\")\n",
        "            start_train = time.time()\n",
        "            model.fit(x_train, y_train)\n",
        "            train_time_minutes = (time.time() - start_train)/60\n",
        "            print(f\"[TRAINING] Finished in {train_time_minutes:.2f} minutes\\n\")\n",
        "\n",
        "            # Testing\n",
        "            print(f\"[TESTING] Start\")\n",
        "            start_test = time.time()\n",
        "            y_train_pred = model.predict(x_train)\n",
        "            y_test_pred = model.predict(x_test)\n",
        "            test_time_minutes = (time.time() - start_test)/60\n",
        "            print(f\"[TESTING] Finished in {test_time_minutes:.2f} minutes\\n\")\n",
        "\n",
        "            # Accuracy\n",
        "            print(\"[ACCURACIES] Calculate\")\n",
        "            train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "            print(f\"[ACCURACIES] calculated => Train Acc: {train_accuracy:.2f} | Test Acc: {test_accuracy:.2f}\\n\")\n",
        "\n",
        "            # Correct / incorrect counts\n",
        "            print(\"[PREDICTIONS] Calculate\")\n",
        "            train_correct, train_incorrect = self.count_correct_incorrect(y_train, y_train_pred)\n",
        "            test_correct, test_incorrect = self.count_correct_incorrect(y_test, y_test_pred)\n",
        "            print(f\"[PREDICTIONS] Calculated (Correct/Incorrect) => Train ({train_correct}/{train_incorrect}) | Test ({test_correct}/{test_incorrect})\\n\")\n",
        "\n",
        "            # Divider\n",
        "            print(f\"{150 * '-'}\\n\")\n",
        "\n",
        "            # Add data in results list\n",
        "            self.results.append({\n",
        "                **params,\n",
        "                \"train_accuracy\": train_accuracy,\n",
        "                \"test_accuracy\": test_accuracy,\n",
        "                \"train_time\": train_time_minutes,\n",
        "                \"test_time\": test_time_minutes,\n",
        "                \"train_correct\": train_correct,\n",
        "                \"train_incorrect\": train_incorrect,\n",
        "                \"test_correct\": test_correct,\n",
        "                \"test_incorrect\": test_incorrect\n",
        "            })\n",
        "\n",
        "            # Keep track of best model\n",
        "            if test_accuracy > best_test_accuracy:\n",
        "\n",
        "                best_test_accuracy = test_accuracy\n",
        "\n",
        "                # Store the best model along with extra info\n",
        "                self.__model = {\n",
        "                    \"model\": model,\n",
        "                    \"train_accuracy\": train_accuracy,\n",
        "                    \"test_accuracy\": test_accuracy,\n",
        "                    \"train_time\": train_time_minutes,\n",
        "                    \"test_time\": test_time_minutes,\n",
        "                    \"train_correct\": train_correct,\n",
        "                    \"train_incorrect\": train_incorrect,\n",
        "                    \"test_correct\": test_correct,\n",
        "                    \"test_incorrect\": test_incorrect,\n",
        "                    \"params\": params,\n",
        "                    \"y_test_pred\": y_test_pred\n",
        "                }\n",
        "\n",
        "        # Store data to file\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(self.results)\n",
        "        # Build timestamped filename\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"svm_full_results_{timestamp}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Saved all model results to {filename}\\n\")\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Helper Class"
      ],
      "metadata": {
        "id": "OILq0eEgoFzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "class DataHelper:\n",
        "\n",
        "    @staticmethod\n",
        "    def load_cifar10_data():\n",
        "\n",
        "        # Load data from CIFAR10\n",
        "        print(\"[LOAD CIFAR10] Start...\")\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        print(\"[LOAD CIFAR10] Finished\\n\")\n",
        "\n",
        "        print(f\"Training set: {x_train.shape}\")\n",
        "        print(f\"Testing set: {x_test.shape}\\n\")\n",
        "\n",
        "        return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_load_mat(path):\n",
        "        data = loadmat(path)\n",
        "        x = data['X']  # shape: (32, 32, 3, N)\n",
        "        y = data['y'].squeeze()  # shape: (N,)\n",
        "\n",
        "        # Move images to (N, 32, 32, 3)\n",
        "        x = np.transpose(x, (3, 0, 1, 2))\n",
        "\n",
        "        # SVHN uses label \"10\" to represent \"0\"\n",
        "        y[y == 10] = 0\n",
        "        return x, y\n",
        "\n",
        "    @staticmethod\n",
        "    def load_svhn_data():\n",
        "\n",
        "        # Load data from CIFAR10\n",
        "        print(\"[LOAD SVHN] Start...\")\n",
        "\n",
        "        # Load all sets\n",
        "        x_train, y_train = DataHelper.custom_load_mat(\"./svhn_data/train_32x32.mat\")\n",
        "        x_test, y_test = DataHelper.custom_load_mat(\"./svhn_data/test_32x32.mat\")\n",
        "        # x_extra, y_extra = DataHelper.custom_load_mat(\"./svhn_data/extra_32x32.mat\")\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        x_train = x_train.astype(\"float32\") / 255.0\n",
        "        x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "        print(\"[LOAD SVHN] Finished\\n\")\n",
        "\n",
        "        print(f\"Training set: {x_train.shape}\")\n",
        "        print(f\"Testing set: {x_test.shape}\\n\")\n",
        "\n",
        "        return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "    @staticmethod\n",
        "    def minimize_samples(x_train, x_test, y_train, y_test, max_train_samples=4000, max_test_samples=500):\n",
        "\n",
        "        print(\"[MINIMIZE SAMPLES] Start...\")\n",
        "        np.random.seed(0)\n",
        "        train_idx = np.random.choice(len(x_train), max_train_samples, replace=False)\n",
        "        test_idx = np.random.choice(len(x_test), max_test_samples, replace=False)\n",
        "        print(\"[MINIMIZE SAMPLES] Finished\\n\")\n",
        "\n",
        "        return x_train[train_idx], x_test[test_idx], y_train[train_idx], y_test[test_idx]\n",
        "\n",
        "    # Flatten images to vectors\n",
        "    @staticmethod\n",
        "    def flatten_samples(given_x_train, given_x_test):\n",
        "        x_train = given_x_train.reshape(given_x_train.shape[0], -1)\n",
        "        x_test = given_x_test.reshape(given_x_test.shape[0], -1)\n",
        "        return x_train, x_test\n",
        "\n",
        "    # Remove extra label dimension  (shape becomes (N,) instead of (N,1))\n",
        "    @staticmethod\n",
        "    def ravel_samples(given_y_train, given_y_test):\n",
        "        y_train = given_y_train.ravel()\n",
        "        y_test = given_y_test.ravel()\n",
        "        return y_train, y_test\n",
        "\n",
        "    @staticmethod\n",
        "    def pca_samples(given_x_train, given_x_test):\n",
        "\n",
        "        print(\"[PCA] Apply...\")\n",
        "        pca = PCA(n_components=0.90)\n",
        "        x_train = pca.fit_transform(given_x_train)\n",
        "        x_test = pca.transform(given_x_test)\n",
        "        print(\"[PCA] Applied\\n\")\n",
        "\n",
        "        print(f\"Training set after PCA: {x_train.shape}\")\n",
        "        print(f\"Testing set after PCA: {x_test.shape}\\n\")\n",
        "\n",
        "        return x_train, x_test\n",
        "\n",
        "    @staticmethod\n",
        "    def flatten_and_pca_samples(given_x_train, given_x_test):\n",
        "\n",
        "        # Flatten images to vectors before PCA\n",
        "        x_train, x_test = DataHelper.flatten_samples(given_x_train, given_x_test)\n",
        "\n",
        "        # Execute PCA in samples\n",
        "        x_train, x_test = DataHelper.pca_samples(x_train, x_test)\n",
        "\n",
        "        return x_train, x_test\n",
        "\n",
        "    @staticmethod\n",
        "    def show_grid_examples(indices, images, y_true, y_pred, title):\n",
        "\n",
        "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        for idx, sample_idx in enumerate(indices):\n",
        "            img = images[sample_idx].reshape(32, 32, 3)\n",
        "\n",
        "            true_label = class_names[int(y_true[sample_idx])]\n",
        "            pred_label = class_names[int(y_pred[sample_idx])]\n",
        "\n",
        "            plt.subplot(1, len(indices), idx + 1)\n",
        "            plt.imshow(img.astype(np.uint8))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"T: {true_label}\\nP: {pred_label}\")\n",
        "\n",
        "        plt.suptitle(title, fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def display_grid_with_random_indices(indexes, size, original_min_x_test, min_y_test, class_y_test_pred, tile):\n",
        "        random_indexes = np.random.choice(indexes, size=size, replace=False)\n",
        "        DataHelper.show_grid_examples(\n",
        "            random_indexes,\n",
        "            original_min_x_test, min_y_test, class_y_test_pred,\n",
        "            tile\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def random_visualization(correct_size, incorrect_size, model_info, desired_y_test, original_x_test):\n",
        "\n",
        "        class_y_test_pred = model_info[\"y_test_pred\"]\n",
        "\n",
        "        # Find correct and incorrect indices\n",
        "        correct_idx = np.where(class_y_test_pred == desired_y_test)[0]\n",
        "        incorrect_idx = np.where(class_y_test_pred != desired_y_test)[0]\n",
        "\n",
        "        # Random correct classifications\n",
        "        DataHelper.display_grid_with_random_indices(\n",
        "            correct_idx, correct_size,\n",
        "            original_x_test, desired_y_test, class_y_test_pred,\n",
        "            f\"{correct_size} Correct Classifications\"\n",
        "        )\n",
        "\n",
        "        # Random incorrect classifications\n",
        "        DataHelper.display_grid_with_random_indices(\n",
        "            incorrect_idx, incorrect_size,\n",
        "            original_x_test, desired_y_test, class_y_test_pred,\n",
        "            f\"{incorrect_size} Incorrect Classifications\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "R-CmgnAJoEUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "vgfBFYlDoKmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    minimize_samples = True\n",
        "\n",
        "    # Load data from CIFAR10\n",
        "    (x_train, y_train), (x_test, y_test) = DataHelper.load_cifar10_data()\n",
        "    # (x_train, y_train), (x_test, y_test) = DataHelper.load_svhn_data()\n",
        "\n",
        "    if minimize_samples:\n",
        "\n",
        "        # Minimize samples\n",
        "        min_x_train, min_x_test, min_y_train, min_y_test = DataHelper.minimize_samples(\n",
        "            x_train, x_test, y_train, y_test,\n",
        "            max_train_samples=5000,\n",
        "            max_test_samples=835\n",
        "        )\n",
        "\n",
        "        desired_x_train, desired_x_test, desired_y_train, desired_y_test = min_x_train, min_x_test, min_y_train, min_y_test\n",
        "\n",
        "    else:\n",
        "\n",
        "        desired_x_train, desired_x_test, desired_y_train, desired_y_test = x_train, x_test, y_train, y_test\n",
        "\n",
        "    # # Save original minimized x test samples\n",
        "    # original_x_test = desired_x_test.copy()\n",
        "\n",
        "    # Flatten and PCA\n",
        "    desired_x_train, desired_x_test = DataHelper.flatten_and_pca_samples(given_x_train=desired_x_train, given_x_test=desired_x_test)\n",
        "\n",
        "    # Remove extra label dimension  (shape becomes (N,) instead of (N,1))\n",
        "    desired_y_train, desired_y_test = DataHelper.ravel_samples(given_y_train=desired_y_train, given_y_test=desired_y_test)\n",
        "\n",
        "    # Create svm classifier\n",
        "    svm_classifier = SVMClassifier()\n",
        "    df = svm_classifier.evaluate_all_models(\n",
        "        desired_x_train, desired_x_test,\n",
        "        desired_y_train, desired_y_test\n",
        "    )\n",
        "\n",
        "    # Print data frame\n",
        "    print(\"All model results:\\n\\n\")\n",
        "    print(df)\n",
        "\n",
        "    # Get desired model information\n",
        "    best_model_info = svm_classifier.get_model()\n",
        "\n",
        "    # Print best model information\n",
        "    print(f\"\\n\\nBest model:\\n\\n{best_model_info}\\n\")\n",
        "\n",
        "    # # Random visualization\n",
        "    # DataHelper.random_visualization(3, 3, best_model_info, desired_y_test, original_x_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Start datetime\n",
        "    start_datetime = datetime.now()\n",
        "    print(f\"\\n[TIME] Start datetime: {start_datetime.strftime('%d/%m/%Y %H:%M:%S')}\\n\")\n",
        "\n",
        "    # Execute main\n",
        "    main()\n",
        "\n",
        "    # End datetime\n",
        "    end_datetime = datetime.now()\n",
        "    print(f\"[TIME] End datetime: {end_datetime.strftime('%d/%m/%Y %H:%M:%S')}\\n\")\n",
        "\n",
        "    # Execution time\n",
        "    execution_time_minutes = (end_datetime - start_datetime).total_seconds()/60\n",
        "    print(f\"[TIME] Execution time: {execution_time_minutes:.2f} minutes\\n\")"
      ],
      "metadata": {
        "id": "xAuGNQVooNpP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}